{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. PubMed\n",
    "\n",
    "Search PubMed for papers\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pubmed/\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/books/NBK25499/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lcp.reuse as reuse\n",
    "from Bio import Entrez\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'eICU' # 'MIMIC'\n",
    "\n",
    "def full_query(base_query, restriction_query):\n",
    "    return ' AND '.join([base_query, restriction_query])\n",
    "\n",
    "entrez_email = 'mimic-support@physionet.org'\n",
    "if PROJECT == 'eICU':\n",
    "    # Retrieves publications which cite this reference\n",
    "    PMID = '30204154'\n",
    "\n",
    "mimic_query = '(mimic-ii OR mimic-iii OR mimicii OR mimiciii OR mimic-2 OR mimic-3 OR mimic2 OR mimic3)'\n",
    "if PROJECT == 'MIMIC':\n",
    "    base_query = mimic_query\n",
    "elif PROJECT == 'eICU':\n",
    "    base_query = f'(eicu-crd OR \"eICU Collaborative Research Database\" OR (eicu AND ({mimic_query} OR database OR MIT OR Phillips)))'\n",
    "# Other terms added to remove false positives. The more terms added without increasing FPs, the better.\n",
    "if PROJECT == 'MIMIC':\n",
    "    restriction_query = '(physionet OR icu OR \"intensive care\" OR \"critical care\")'\n",
    "elif PROJECT == 'eICU':\n",
    "    restriction_query = '(\"intensive care\" OR \"critical care\")'\n",
    "# more restriction keyword ideas: clinical, database, waveform (not suitable on their own due to general mimic term)\n",
    "full_query = full_query(base_query, restriction_query)\n",
    "\n",
    "if PMID:\n",
    "    search_strings = [\n",
    "        'PMID'\n",
    "    ]\n",
    "else:\n",
    "    search_strings = [\n",
    "        base_query#,\n",
    "        # full_query\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_results = reuse.search_list(search_strings, entrez_email)\n",
    "if PMID:\n",
    "    citations = []\n",
    "    pmc_ids = Entrez.read(Entrez.elink(dbfrom='pubmed', db='pmc', LinkName='pubmed_pmc_refs', from_uid=PMID))\n",
    "    pmc_ids = [list(i.values())[0] for i in pmc_ids[0]['LinkSetDb'][0]['Link']]\n",
    "    pm_ids = Entrez.read(Entrez.elink(dbfrom='pmc', db='pubmed', LinkName='pmc_pubmed', from_uid=','.join(pmc_ids)))\n",
    "    for pm_id in [list(i.values())[0] for i in pm_ids[0]['LinkSetDb'][0]['Link']]:\n",
    "        citations.append(pm_id)\n",
    "    search_results_all = reuse.search_list(entrez_email, ids=citations, all=True)\n",
    "else:\n",
    "    search_results_all = reuse.search_list(entrez_email, search_strings=search_strings, all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of results\n",
    "if PMID:\n",
    "    result = search_results_all['PMID']\n",
    "    print(f'PMID:\\n - Count: {len(result.index)}')\n",
    "else:\n",
    "    for ss in search_strings:\n",
    "        result = search_results_all[ss]\n",
    "        print(f'{ss}:\\n - Count: {len(result.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the titles to files\n",
    "write_dir = os.path.join('search_results', PROJECT, 'pubmed')\n",
    "os.makedirs(write_dir, exist_ok=True)\n",
    "\n",
    "if PMID:\n",
    "    search_results_all['PMID'].to_csv(os.path.join(write_dir, 'PMID-citations-all.csv'), index=False)\n",
    "else:\n",
    "    search_results_all[base_query].to_csv(os.path.join(write_dir, 'without-constraints-all.csv'), index=False)\n",
    "    search_results_all[full_query].to_csv(os.path.join(write_dir, 'with-constraints-all.csv'), index=False)\n",
    "\n",
    "# base_query_file = os.path.join(write_dir, 'without-constraints.txt')\n",
    "# with open(base_query_file, 'w') as f:\n",
    "#     for line in search_results[base_query].paper_titles:\n",
    "#         f.write(line+'\\n')\n",
    "\n",
    "# full_query_file = os.path.join(write_dir, 'with-constraints.txt')\n",
    "# with open(full_query_file, 'w') as f:\n",
    "#     for line in search_results[full_query].paper_titles:\n",
    "#         f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new one for each year\n",
    "if PMID:\n",
    "    all_years = sorted(set(list(search_results_all['PMID']['Publication Year'])))\n",
    "    for year in all_years:\n",
    "        year_df = search_results_all['PMID'][search_results_all['PMID']['Publication Year'] == year]\n",
    "        year_df.to_csv(os.path.join(write_dir, f'PMID-citations-all_{year}.csv'), index=False)\n",
    "else:\n",
    "    all_years = sorted(set(list(search_results_all[full_query]['Publication Year'])))\n",
    "    for year in all_years:\n",
    "        year_df_without = search_results_all[base_query][search_results_all[base_query]['Publication Year'] == year]\n",
    "        year_df_with = search_results_all[full_query][search_results_all[full_query]['Publication Year'] == year]\n",
    "        year_df_without.to_csv(os.path.join(write_dir, f'without-constraints-all_{year}.csv'), index=False)\n",
    "        year_df_with.to_csv(os.path.join(write_dir, f'with-constraints-all_{year}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of number of publications each year\n",
    "if PMID:\n",
    "    all_years = sorted(set(list(search_results_all['PMID']['Publication Year'])))\n",
    "\n",
    "    years = []\n",
    "    for year in all_years:\n",
    "        year_df = search_results_all['PMID'][search_results_all['PMID']['Publication Year'] == year]\n",
    "\n",
    "        for _ in range(len(year_df.index)):\n",
    "            years.append(year)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(years, facecolor='k', edgecolor='w', bins=np.arange(len(all_years)+1)-0.5)\n",
    "    plt.savefig(os.path.join(write_dir, 'PMID-citations_histogram.jpg'))\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Number of Publications', fontsize=12)\n",
    "    plt.show()\n",
    "else:\n",
    "    all_years = sorted(set(list(search_results_all[full_query]['Publication Year'])))\n",
    "\n",
    "    year_without = []\n",
    "    year_with = []\n",
    "    for year in all_years:\n",
    "        year_df_without = search_results_all[base_query][search_results_all[base_query]['Publication Year'] == year]\n",
    "        year_df_with = search_results_all[full_query][search_results_all[full_query]['Publication Year'] == year]\n",
    "\n",
    "        for _ in range(len(year_df_without.index)):\n",
    "            year_without.append(year)\n",
    "        for _ in range(len(year_df_with.index)):\n",
    "            year_with.append(year)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(year_without, facecolor='k', edgecolor='w', bins=np.arange(len(all_years)+1)-0.5)\n",
    "    plt.savefig(os.path.join(write_dir, 'without-constraints_histogram.jpg'))\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Number of Publications', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(year_with, facecolor='k', edgecolor='w', bins=np.arange(len(all_years)+1)-0.5)\n",
    "    plt.savefig(os.path.join(write_dir, 'with-constraints_histogram.jpg'))\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Number of Publications', fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO: Create a file called without-constraints-inspected.tsv and mark the second column with T/F for true/false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the effect of restricting the search by additional criteria.\n",
    "# The differences show that many false positives, and a few true positives, are removed.\n",
    "reuse.showdiff(search_results[search_strings[0]],\n",
    "               search_results[search_strings[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the labelled results for the general unconstrained search query\n",
    "labelled_results = pd.read_csv(os.path.join(write_dir, 'without-constraints-inspected.tsv'), delimiter='\\t', header=None)\n",
    "false_positives = labelled_results.loc[labelled_results[1]=='F'][0].values\n",
    "true_positives = labelled_results.loc[labelled_results[1]=='T'][0].values\n",
    "print('Number of results found using the unconstrained search term:', len(labelled_results))\n",
    "print('Number of false positives:',len(false_positives))\n",
    "print('Number of true positives:',len(true_positives))\n",
    "\n",
    "constrained_titles = search_results[search_strings[1]].paper_titles\n",
    "print('\\nCompare ^ true positives with:')\n",
    "print('Number of results from the constrained search term:', len(constrained_titles))\n",
    "\n",
    "missed_papers = set(true_positives) - set(constrained_titles)\n",
    "print('Number of missed true positives:', len(missed_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at some True positives missed by the constrained search term to figure out what else you can add.\n",
    "# Write to a file to label comments.\n",
    "write_dir = os.path.join('search_results', 'pubmed')\n",
    "\n",
    "missed_papers_file = os.path.join(write_dir, 'missed-papers.tsv')\n",
    "\n",
    "with open(missed_papers_file, 'w') as f:\n",
    "    for line in missed_papers:\n",
    "        f.write(line+'\\n')\n",
    "\n",
    "display(missed_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Web of Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import time\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "base_mimic_query = '(mimic-ii OR mimic-iii OR mimicii OR mimiciii OR mimic-2 OR mimic-3 OR mimic2 OR mimic3)'\n",
    "restriction_query = '(physionet OR icu OR “intensive care” OR “critical care”)'\n",
    "def full_query(base_query, restriction_query):\n",
    "    return ' AND '.join([base_query, restriction_query])\n",
    "full_mimic_query = full_query(base_mimic_query, restriction_query)\n",
    "#base_search_url = 'https://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=2F46AeWkMQBRAZlzDWm&preferencesSaved='\n",
    "base_search_url = 'https://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=1AnC2UMojuKrtrl7T5R&preferencesSaved='\n",
    "\n",
    "all_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to the search page\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(base_search_url)\n",
    "\n",
    "# Input the query string\n",
    "time.sleep(2.5)\n",
    "searchbox = driver.find_element_by_id('value(input1)')\n",
    "searchbox.send_keys(full_mimic_query)\n",
    "\n",
    "# Search\n",
    "time.sleep(1)\n",
    "searchbutton = driver.find_element_by_css_selector('.standard-button.primary-button.large-search-button')\n",
    "searchbutton.click()\n",
    "\n",
    "# Get the total number of pages\n",
    "npages = int(driver.find_element_by_id('pageCount.top').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles!!!\n",
    "while True:\n",
    "    # Get the current page number\n",
    "    pagenum = int(driver.find_element_by_class_name('goToPageNumber-input').get_property('value'))\n",
    "\n",
    "    # Get the titles. This also captures the journals. So every second value is not a title.\n",
    "    elements = driver.find_elements_by_class_name('smallV110')\n",
    "\n",
    "    for e in elements[::2]:\n",
    "        all_titles.append(e.find_element_by_tag_name('value').text)\n",
    "        \n",
    "    if pagenum < npages:\n",
    "        nextbutton = driver.find_element_by_class_name('paginationNext')\n",
    "        nextbutton.click()\n",
    "    else:\n",
    "        print('Got all paper titles!')\n",
    "        driver.close()\n",
    "        break\n",
    "        \n",
    "all_titles = set(all_titles)\n",
    "#all_titles.remove('')\n",
    "all_titles = [t.lower() for t in list(all_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the titles to files\n",
    "write_dir = os.path.join('search_results/wos')\n",
    "\n",
    "full_query_file = os.path.join(write_dir, 'with-constraints.txt')\n",
    "\n",
    "with open(full_query_file, 'w') as f:\n",
    "    for line in all_titles:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SCOPUS\n",
    "\n",
    "Shit search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. IEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import time\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "base_mimic_query = '(mimic-ii OR mimic-iii OR mimicii OR mimiciii OR mimic-2 OR mimic-3 OR mimic2 OR mimic3)'\n",
    "restriction_query = '(physionet OR icu OR “intensive care” OR “critical care”)'\n",
    "def full_query(base_query, restriction_query):\n",
    "    return ' AND '.join([base_query, restriction_query])\n",
    "full_mimic_query = full_query(base_mimic_query, restriction_query)\n",
    "base_search_url = 'http://ieeexplore.ieee.org/search/advsearch.jsp?expression-builder'\n",
    "all_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to the search page\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(base_search_url)\n",
    "\n",
    "# Input the query string\n",
    "searchbox = driver.find_element_by_id('expression-textarea')\n",
    "searchbox.send_keys(full_mimic_query)\n",
    "# Select the 'full text and metadata' box\n",
    "radiobutton = driver.find_element_by_id('Search_All_Text')\n",
    "radiobutton.click()\n",
    "\n",
    "# Search\n",
    "time.sleep(1)\n",
    "searchbutton = driver.find_element_by_class_name('stats-Adv_Command_search')\n",
    "searchbutton.click()\n",
    "\n",
    "# Get the total number of pages\n",
    "#npages = int(driver.find_element_by_id('pageCount.top').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles!!!\n",
    "while True:\n",
    "    # let the page load\n",
    "    time.sleep(2)\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(0.5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "    # Get the titles.\n",
    "    # They are in: <h2 class=\"result-item-title\"><a class=\"ng-binding ng-scope\">title</a></h2>\n",
    "    elements = driver.find_elements_by_class_name('result-item-title')\n",
    "    for e in elements:\n",
    "        # Text may appear with \"[::sometext::]\"\n",
    "        all_titles.append(e.find_element_by_tag_name('a').get_attribute('text').replace('[::', '').replace('::]', ''))\n",
    "        # New line separated journal info and such\n",
    "        #all_titles.append(e.text.split('\\n')[0])\n",
    "    \n",
    "    # Click next page if any\n",
    "    \n",
    "    e = driver.find_element_by_class_name('next')\n",
    "    if 'disabled' in e.get_attribute('class'):\n",
    "        print('Got all paper titles!')\n",
    "        driver.close()\n",
    "        break\n",
    "    else:\n",
    "        nextbutton = driver.find_element_by_link_text('>')\n",
    "        nextbutton.click()\n",
    "\n",
    "all_titles = set(all_titles)\n",
    "all_titles = [t.lower() for t in list(all_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_titles))\n",
    "display(all_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the titles to files\n",
    "write_dir = os.path.join('search_results/ieee')\n",
    "\n",
    "full_query_file = os.path.join(write_dir, 'with-constraints.txt')\n",
    "\n",
    "with open(full_query_file, 'w') as f:\n",
    "    for line in all_titles:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Results - pubmed, wos, ieee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'search_results'\n",
    "\n",
    "combined_results = []\n",
    "for service in ['pubmed', 'wos', 'ieee']:\n",
    "    # For pubmed, get the curated true positives from the unconstrained search instead\n",
    "    if service == 'pubmed':\n",
    "        df = pd.read_csv(os.path.join(result_dir, service, 'without-constraints-inspected.tsv'), delimiter='\\t', header=None)\n",
    "        service_results = list(df.loc[df[1]=='T'][0].values)\n",
    "    # For other services, get the constrained search results\n",
    "    else:\n",
    "        with open(os.path.join(result_dir, service, 'with-constraints.txt')) as f:\n",
    "            service_results = f.readlines()\n",
    "    print('Number of results from service '+service+': '+str(len(service_results)))\n",
    "    combined_results = combined_results + [r.strip() for r in service_results]\n",
    "\n",
    "print('\\nTotal number of non-unique results: ', len(combined_results))\n",
    "combined_results = sorted(list(set(combined_results)))\n",
    "print('Total number of unique results: ', len(combined_results))\n",
    "with open(os.path.join(result_dir, 'combined', 'with-constraints.txt'), 'w') as f:\n",
    "    for r in combined_results:\n",
    "        f.write(r+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may 21 2018\n",
    "Number of results from service pubmed: 155\n",
    "Number of results from service wos: 152\n",
    "Number of results from service ieee: 322\n",
    "\n",
    "Total number of non-unique results:  629\n",
    "Total number of unique results:  456\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to parse GS automatically failed. Below is evidence of failure. Can ignore..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N. Search Google Scholar\n",
    "\n",
    "Packages found online:\n",
    "- https://github.com/ckreibich/scholar.py\n",
    "- https://github.com/venthur/gscholar\n",
    "- https://github.com/adeel/google-scholar-scraper\n",
    "- http://code.activestate.com/recipes/523047-search-google-scholar/\n",
    "- https://github.com/erdiaker/torrequest\n",
    "- https://github.com/NikolaiT/GoogleScraper\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python\n",
    "\n",
    "\n",
    "Query: `(\"mimic ii\" OR \"mimic iii\") AND (\"database\" OR \"clinical\" OR \"waveform\" OR ICU)`\n",
    "\n",
    "https://scholar.google.com/scholar?q=%28mimic-ii+OR+mimic-iii%29&btnG=&hl=en&as_sdt=1%2C22&as_vis=1\n",
    "\n",
    "https://scholar.google.com/scholar/help.html\n",
    "\n",
    "\n",
    "https://superuser.com/questions/565722/how-to-config-tor-to-use-a-http-socks-proxy\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Browse with JS enabled. requests library uses http. Otherwise google will think (correctly) that you are a robot.\n",
    "2. Change IP every time, or google will block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'eICU'\n",
    "\n",
    "# Search by Google Scholar publication ID\n",
    "search_results = []\n",
    "if PROJECT == 'eICU':\n",
    "    try:\n",
    "        search_results.append(scholarly.search_citedby(11878669525996073977))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "total_results = search_results[0].total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pubs = list(search_results)\n",
    "all_pubs = []\n",
    "for i in range(total_results):\n",
    "    print(100*i/total_results)\n",
    "    try:\n",
    "        ct = next(search_results)\n",
    "        all_pubs.append({\n",
    "            'title': ct['bib']['title'],\n",
    "            'pub_year': ct['bib']['pub_year']\n",
    "        })\n",
    "    except:\n",
    "        pass\n",
    "# print(all_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
